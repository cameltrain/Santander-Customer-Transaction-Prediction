{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font size=\"6\">Santander EDA, PCA and Light GBM Classification Model</font></center></h1>\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\"></img>\n",
    "\n",
    "<br>\n",
    "<b>\n",
    "In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem. \n",
    "The data is anonimyzed, each row containing 200 numerical values identified just with a number.</b>\n",
    "\n",
    "<b>Inspired by Jiwei Liu's Kernel. I added Data Augmentation Segment to my kernel</b>\n",
    "\n",
    "### I will not be covering EDA in this kernel . I'd keep it short as the data is completely anonimized and all columns are just pure numbers, giving almost no insight . \n",
    "https://www.kaggle.com/roydatascience/eda-pca-lgbm-santander-transactions  You can check for EDA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "df_train = pd.read_csv('../../data/train.csv')\n",
    "df_test = pd.read_csv('../../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_code', 'target', 'var_0', 'var_1', 'var_2', 'var_3', 'var_4',\n",
       "       'var_5', 'var_6', 'var_7',\n",
       "       ...\n",
       "       'var_190', 'var_191', 'var_192', 'var_193', 'var_194', 'var_195',\n",
       "       'var_196', 'var_197', 'var_198', 'var_199'],\n",
       "      dtype='object', length=202)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total values in the dataset : 200000\n",
      "% of 1s in total 10.049\n"
     ]
    }
   ],
   "source": [
    "print(\"Total values in the dataset : {}\".format(df_train['target'].count()))\n",
    "Ones = df_train.groupby('target')['target'].count()\n",
    "print(\"% of 1s in total {}\".format(Ones[1]*100.0/200000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As one can see , there is a class imbalance. \n",
    "### Now , how do we solve it ? \n",
    "\n",
    "### Plan 1 : Oversampling / Undersampling -> \n",
    "* In this strategy , we either increase or decrease the number of samples by duplicating the smaller class or removing the majority class elements to make them equal or similar\n",
    "* The risk involved is that we may change the original distribution of the data . \n",
    "\n",
    "### Plan 2 : Follow below ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we filter using masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_13</td>\n",
       "      <td>1</td>\n",
       "      <td>16.3699</td>\n",
       "      <td>1.5934</td>\n",
       "      <td>16.7395</td>\n",
       "      <td>7.3330</td>\n",
       "      <td>12.1450</td>\n",
       "      <td>5.9004</td>\n",
       "      <td>4.8222</td>\n",
       "      <td>20.9729</td>\n",
       "      <td>...</td>\n",
       "      <td>7.4002</td>\n",
       "      <td>7.4031</td>\n",
       "      <td>4.3989</td>\n",
       "      <td>4.0978</td>\n",
       "      <td>17.3638</td>\n",
       "      <td>-1.3022</td>\n",
       "      <td>9.6846</td>\n",
       "      <td>9.0419</td>\n",
       "      <td>15.6064</td>\n",
       "      <td>-10.8529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>train_29</td>\n",
       "      <td>1</td>\n",
       "      <td>5.3301</td>\n",
       "      <td>-2.6064</td>\n",
       "      <td>13.1913</td>\n",
       "      <td>3.1193</td>\n",
       "      <td>6.6483</td>\n",
       "      <td>-6.5659</td>\n",
       "      <td>5.9064</td>\n",
       "      <td>15.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9215</td>\n",
       "      <td>7.9676</td>\n",
       "      <td>2.3405</td>\n",
       "      <td>1.1482</td>\n",
       "      <td>23.2168</td>\n",
       "      <td>-2.0105</td>\n",
       "      <td>3.7600</td>\n",
       "      <td>9.4513</td>\n",
       "      <td>17.4105</td>\n",
       "      <td>-14.6897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>train_63</td>\n",
       "      <td>1</td>\n",
       "      <td>7.7072</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>9.9974</td>\n",
       "      <td>8.3524</td>\n",
       "      <td>9.2886</td>\n",
       "      <td>-13.3627</td>\n",
       "      <td>6.0425</td>\n",
       "      <td>10.1108</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5832</td>\n",
       "      <td>5.0039</td>\n",
       "      <td>3.8814</td>\n",
       "      <td>7.4241</td>\n",
       "      <td>21.4844</td>\n",
       "      <td>-0.8297</td>\n",
       "      <td>-3.0468</td>\n",
       "      <td>7.5790</td>\n",
       "      <td>15.7685</td>\n",
       "      <td>5.4769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>train_65</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5358</td>\n",
       "      <td>-2.5439</td>\n",
       "      <td>8.7394</td>\n",
       "      <td>6.7548</td>\n",
       "      <td>14.4099</td>\n",
       "      <td>-3.8724</td>\n",
       "      <td>5.1584</td>\n",
       "      <td>15.8381</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6648</td>\n",
       "      <td>6.4227</td>\n",
       "      <td>3.4025</td>\n",
       "      <td>-4.0882</td>\n",
       "      <td>14.1174</td>\n",
       "      <td>-0.2472</td>\n",
       "      <td>5.3847</td>\n",
       "      <td>8.6949</td>\n",
       "      <td>15.1340</td>\n",
       "      <td>3.8449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>train_71</td>\n",
       "      <td>1</td>\n",
       "      <td>6.7547</td>\n",
       "      <td>2.5973</td>\n",
       "      <td>14.2141</td>\n",
       "      <td>8.3514</td>\n",
       "      <td>7.4942</td>\n",
       "      <td>-1.3055</td>\n",
       "      <td>4.2336</td>\n",
       "      <td>15.0243</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4677</td>\n",
       "      <td>3.5935</td>\n",
       "      <td>2.0013</td>\n",
       "      <td>1.5777</td>\n",
       "      <td>18.2820</td>\n",
       "      <td>-4.3408</td>\n",
       "      <td>6.8869</td>\n",
       "      <td>9.3567</td>\n",
       "      <td>18.9013</td>\n",
       "      <td>13.3447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_code  target    var_0   var_1    var_2   var_3    var_4    var_5  \\\n",
       "13  train_13       1  16.3699  1.5934  16.7395  7.3330  12.1450   5.9004   \n",
       "29  train_29       1   5.3301 -2.6064  13.1913  3.1193   6.6483  -6.5659   \n",
       "63  train_63       1   7.7072  0.0183   9.9974  8.3524   9.2886 -13.3627   \n",
       "65  train_65       1  10.5358 -2.5439   8.7394  6.7548  14.4099  -3.8724   \n",
       "71  train_71       1   6.7547  2.5973  14.2141  8.3514   7.4942  -1.3055   \n",
       "\n",
       "     var_6    var_7  ...  var_190  var_191  var_192  var_193  var_194  \\\n",
       "13  4.8222  20.9729  ...   7.4002   7.4031   4.3989   4.0978  17.3638   \n",
       "29  5.9064  15.2341  ...   5.9215   7.9676   2.3405   1.1482  23.2168   \n",
       "63  6.0425  10.1108  ...   1.5832   5.0039   3.8814   7.4241  21.4844   \n",
       "65  5.1584  15.8381  ...   4.6648   6.4227   3.4025  -4.0882  14.1174   \n",
       "71  4.2336  15.0243  ...   1.4677   3.5935   2.0013   1.5777  18.2820   \n",
       "\n",
       "    var_195  var_196  var_197  var_198  var_199  \n",
       "13  -1.3022   9.6846   9.0419  15.6064 -10.8529  \n",
       "29  -2.0105   3.7600   9.4513  17.4105 -14.6897  \n",
       "63  -0.8297  -3.0468   7.5790  15.7685   5.4769  \n",
       "65  -0.2472   5.3847   8.6949  15.1340   3.8449  \n",
       "71  -4.3408   6.8869   9.3567  18.9013  13.3447  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the mask to filter out 1s . \n",
    "y = df_train['target']\n",
    "y.head()\n",
    "x = df_train[y > 0].copy()\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "11e4e9f6ee5aabcb0783768e03e786f141677538"
   },
   "outputs": [],
   "source": [
    "def augment(x,y,t=2):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "2bac2796bb647af39fb8bdfbe295817f7f4c8dce"
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"auc\",\n",
    "    \"boosting\": 'gbdt',\n",
    "    \"max_depth\" : -1,\n",
    "    \"num_leaves\" : 13,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\" : 0.4,\n",
    "    \"feature_fraction\" : 0.05,\n",
    "    \"min_data_in_leaf\": 80,\n",
    "    \"min_sum_hessian_in_leaf\" : 10,\n",
    "    \"tree_learner\": \"serial\",\n",
    "    \"boost_from_average\": \"false\",\n",
    "    #\"lambda_l1\" : 5,\n",
    "    #\"lambda_l2\" : 5,\n",
    "    \"bagging_seed\" : random_state,\n",
    "    \"verbosity\" : 1,\n",
    "    \"seed\": random_state,\n",
    "    'num_threads': 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "d15c7bb080a4642a3b331ae24830cdb6b7b67cde"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "oof = df_train[['ID_code', 'target']]\n",
    "oof['predict'] = 0\n",
    "predictions = df_test[['ID_code']]\n",
    "val_aucs = []\n",
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "20a221b86ee4fe50c371767c63a72f225752213c"
   },
   "outputs": [],
   "source": [
    "features = [col for col in df_train.columns if col not in ['target', 'ID_code']]\n",
    "X_test = df_test[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e48a4e9baa428a0280d526df9d0128f34a7fbe85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.888228\tvalid_1's auc: 0.879911\n",
      "[2000]\ttraining's auc: 0.895354\tvalid_1's auc: 0.886258\n",
      "[3000]\ttraining's auc: 0.90069\tvalid_1's auc: 0.891015\n",
      "[4000]\ttraining's auc: 0.904971\tvalid_1's auc: 0.894502\n",
      "[5000]\ttraining's auc: 0.90819\tvalid_1's auc: 0.896857\n",
      "[6000]\ttraining's auc: 0.91047\tvalid_1's auc: 0.898405\n",
      "[7000]\ttraining's auc: 0.912276\tvalid_1's auc: 0.899375\n",
      "[8000]\ttraining's auc: 0.913778\tvalid_1's auc: 0.900022\n",
      "[9000]\ttraining's auc: 0.915113\tvalid_1's auc: 0.900337\n",
      "[10000]\ttraining's auc: 0.916357\tvalid_1's auc: 0.900458\n",
      "[11000]\ttraining's auc: 0.917562\tvalid_1's auc: 0.900517\n",
      "[12000]\ttraining's auc: 0.918704\tvalid_1's auc: 0.900504\n",
      "[13000]\ttraining's auc: 0.919812\tvalid_1's auc: 0.900404\n",
      "[14000]\ttraining's auc: 0.920905\tvalid_1's auc: 0.900401\n",
      "Early stopping, best iteration is:\n",
      "[11157]\ttraining's auc: 0.917743\tvalid_1's auc: 0.900541\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.888396\tvalid_1's auc: 0.880047\n",
      "[2000]\ttraining's auc: 0.895565\tvalid_1's auc: 0.88641\n",
      "[3000]\ttraining's auc: 0.90097\tvalid_1's auc: 0.891029\n",
      "[4000]\ttraining's auc: 0.905237\tvalid_1's auc: 0.894234\n",
      "[5000]\ttraining's auc: 0.908494\tvalid_1's auc: 0.896662\n",
      "[6000]\ttraining's auc: 0.910799\tvalid_1's auc: 0.898151\n",
      "[7000]\ttraining's auc: 0.912584\tvalid_1's auc: 0.899192\n",
      "[8000]\ttraining's auc: 0.914109\tvalid_1's auc: 0.899783\n",
      "[9000]\ttraining's auc: 0.915449\tvalid_1's auc: 0.900099\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n",
    "    X_train, y_train = df_train.iloc[trn_idx][features], df_train.iloc[trn_idx]['target']\n",
    "    X_valid, y_valid = df_train.iloc[val_idx][features], df_train.iloc[val_idx]['target']\n",
    "    \n",
    "    N = 5\n",
    "    p_valid,yp = 0,0\n",
    "    for i in range(N):\n",
    "        X_t, y_t = augment(X_train.values, y_train.values,12)\n",
    "        X_t = pd.DataFrame(X_t)\n",
    "        X_t = X_t.add_prefix('var_')\n",
    "    \n",
    "        trn_data = lgb.Dataset(X_t, label=y_t)\n",
    "        val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        evals_result = {}\n",
    "        lgb_clf = lgb.train(lgb_params,\n",
    "                        trn_data,\n",
    "                        100000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        early_stopping_rounds=3000,\n",
    "                        verbose_eval = 1000,\n",
    "                        evals_result=evals_result\n",
    "                       )\n",
    "        p_valid += lgb_clf.predict(X_valid)\n",
    "        yp += lgb_clf.predict(X_test)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    oof['predict'][val_idx] = p_valid/N\n",
    "    val_score = roc_auc_score(y_valid, p_valid)\n",
    "    val_aucs.append(val_score)\n",
    "    \n",
    "    predictions['fold{}'.format(fold+1)] = yp/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see what this augmentation does to our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of 1s in original data : {} / {} \".format(np.sum(y_train) , len(y_train)))\n",
    "print(\"Percentage of 1s in original data : {}\".format(np.sum(y_train)*100.0/len(y_train)))\n",
    "\n",
    "\n",
    "print(\"Percentage of 1s in augmented data : {}\".format(np.sum(y_t)*100.0/len(y_t)))\n",
    "print(\"Distribution of 1s in augmented data : {} / {} \".format(np.sum(y_t) , len(y_t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So How the augmentation was done ? \n",
    "\n",
    "    * X : Original  : 200,000\n",
    "    * Xs : Ones     ~  20,000\n",
    "    * Xn : Zeros    ~ 180,000  \n",
    "\n",
    "    ### X_Final = X + 3\\*Xs + 2\\*Xn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof : ? \n",
    "\n",
    "1s = 20k(X) + 3\\*20k(Xs) = 80k\n",
    "Total = 20k(X) + 3\\*20k(Xs) + 2\\*180k(Xn)\n",
    "\n",
    "i.e. , \n",
    "    80k / 520 k = \n",
    "    14.6 % (Approx. , the one we found above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hence this technique is more like oversampling , but , here we oversample BOTH classses , rather than just one.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "055d26ed03bc1b21bade15f91fc7964cebf39fbc"
   },
   "outputs": [],
   "source": [
    "mean_auc = np.mean(val_aucs)\n",
    "std_auc = np.std(val_aucs)\n",
    "all_auc = roc_auc_score(oof['target'], oof['predict'])\n",
    "print(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4a99e866bcecb5cd7655fc21e2fc0409a686268"
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('LightGBM Features (averaged over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f698e19c1c399db90cb98e698a08cc663dd8319"
   },
   "outputs": [],
   "source": [
    "# submission\n",
    "predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\n",
    "predictions.to_csv('lgb_all_predictions.csv', index=None)\n",
    "sub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\n",
    "sub_df[\"target\"] = predictions['target']\n",
    "sub_df.to_csv(\"../data/lgb_submission.csv\", index=False)\n",
    "oof.to_csv('../data/lgb_oof.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
