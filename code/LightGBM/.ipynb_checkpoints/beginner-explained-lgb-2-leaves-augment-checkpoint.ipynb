{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font size=\"6\">Santander EDA, PCA and Light GBM Classification Model</font></center></h1>\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\"></img>\n",
    "\n",
    "<br>\n",
    "<b>\n",
    "In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem. \n",
    "The data is anonimyzed, each row containing 200 numerical values identified just with a number.</b>\n",
    "\n",
    "<b>Inspired by Jiwei Liu's Kernel. I added Data Augmentation Segment to my kernel</b>\n",
    "\n",
    "### I will not be covering EDA in this kernel . I'd keep it short as the data is completely anonimized and all columns are just pure numbers, giving almost no insight . \n",
    "https://www.kaggle.com/roydatascience/eda-pca-lgbm-santander-transactions  You can check for EDA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_code', 'target', 'var_0', 'var_1', 'var_2', 'var_3', 'var_4',\n",
       "       'var_5', 'var_6', 'var_7',\n",
       "       ...\n",
       "       'var_190', 'var_191', 'var_192', 'var_193', 'var_194', 'var_195',\n",
       "       'var_196', 'var_197', 'var_198', 'var_199'],\n",
       "      dtype='object', length=202)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total values in the dataset : 200000\n",
      "% of 1s in total 10.049\n"
     ]
    }
   ],
   "source": [
    "print(\"Total values in the dataset : {}\".format(df_train['target'].count()))\n",
    "Ones = df_train.groupby('target')['target'].count()\n",
    "print(\"% of 1s in total {}\".format(Ones[1]*100.0/200000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As one can see , there is a class imbalance. \n",
    "### Now , how do we solve it ? \n",
    "\n",
    "### Plan 1 : Oversampling / Undersampling -> \n",
    "* In this strategy , we either increase or decrease the number of samples by duplicating the smaller class or removing the majority class elements to make them equal or similar\n",
    "* The risk involved is that we may change the original distribution of the data . \n",
    "\n",
    "### Plan 2 : Follow below ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we filter using masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>...</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_13</td>\n",
       "      <td>1</td>\n",
       "      <td>16.3699</td>\n",
       "      <td>1.5934</td>\n",
       "      <td>16.7395</td>\n",
       "      <td>7.3330</td>\n",
       "      <td>12.1450</td>\n",
       "      <td>5.9004</td>\n",
       "      <td>4.8222</td>\n",
       "      <td>20.9729</td>\n",
       "      <td>1.1064</td>\n",
       "      <td>8.6978</td>\n",
       "      <td>2.3287</td>\n",
       "      <td>-11.3409</td>\n",
       "      <td>13.7999</td>\n",
       "      <td>2.7925</td>\n",
       "      <td>6.3182</td>\n",
       "      <td>14.7313</td>\n",
       "      <td>7.2594</td>\n",
       "      <td>-2.4759</td>\n",
       "      <td>14.3984</td>\n",
       "      <td>9.1793</td>\n",
       "      <td>16.8467</td>\n",
       "      <td>19.4258</td>\n",
       "      <td>1.6565</td>\n",
       "      <td>2.5107</td>\n",
       "      <td>7.1272</td>\n",
       "      <td>13.6444</td>\n",
       "      <td>-12.8761</td>\n",
       "      <td>-1.0136</td>\n",
       "      <td>5.1773</td>\n",
       "      <td>7.3507</td>\n",
       "      <td>-16.0042</td>\n",
       "      <td>6.4460</td>\n",
       "      <td>-3.3238</td>\n",
       "      <td>14.3372</td>\n",
       "      <td>10.9481</td>\n",
       "      <td>6.7386</td>\n",
       "      <td>-0.5110</td>\n",
       "      <td>4.4335</td>\n",
       "      <td>...</td>\n",
       "      <td>32.5287</td>\n",
       "      <td>5.8451</td>\n",
       "      <td>4.9591</td>\n",
       "      <td>11.7805</td>\n",
       "      <td>-7.0936</td>\n",
       "      <td>11.8863</td>\n",
       "      <td>3.2597</td>\n",
       "      <td>-4.3478</td>\n",
       "      <td>7.5711</td>\n",
       "      <td>5.6656</td>\n",
       "      <td>3.5750</td>\n",
       "      <td>-8.6583</td>\n",
       "      <td>20.3131</td>\n",
       "      <td>-2.2723</td>\n",
       "      <td>15.7383</td>\n",
       "      <td>8.1730</td>\n",
       "      <td>-12.4963</td>\n",
       "      <td>15.1929</td>\n",
       "      <td>3.9751</td>\n",
       "      <td>8.5618</td>\n",
       "      <td>2.4566</td>\n",
       "      <td>9.3694</td>\n",
       "      <td>0.6942</td>\n",
       "      <td>11.9694</td>\n",
       "      <td>11.8982</td>\n",
       "      <td>0.6129</td>\n",
       "      <td>10.8123</td>\n",
       "      <td>-1.0803</td>\n",
       "      <td>11.9586</td>\n",
       "      <td>-0.5899</td>\n",
       "      <td>7.4002</td>\n",
       "      <td>7.4031</td>\n",
       "      <td>4.3989</td>\n",
       "      <td>4.0978</td>\n",
       "      <td>17.3638</td>\n",
       "      <td>-1.3022</td>\n",
       "      <td>9.6846</td>\n",
       "      <td>9.0419</td>\n",
       "      <td>15.6064</td>\n",
       "      <td>-10.8529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>train_29</td>\n",
       "      <td>1</td>\n",
       "      <td>5.3301</td>\n",
       "      <td>-2.6064</td>\n",
       "      <td>13.1913</td>\n",
       "      <td>3.1193</td>\n",
       "      <td>6.6483</td>\n",
       "      <td>-6.5659</td>\n",
       "      <td>5.9064</td>\n",
       "      <td>15.2341</td>\n",
       "      <td>1.2915</td>\n",
       "      <td>9.1168</td>\n",
       "      <td>6.8278</td>\n",
       "      <td>1.3898</td>\n",
       "      <td>14.0957</td>\n",
       "      <td>12.3335</td>\n",
       "      <td>9.0616</td>\n",
       "      <td>14.8726</td>\n",
       "      <td>11.3542</td>\n",
       "      <td>5.2004</td>\n",
       "      <td>1.6610</td>\n",
       "      <td>24.3097</td>\n",
       "      <td>3.1983</td>\n",
       "      <td>13.0446</td>\n",
       "      <td>4.3067</td>\n",
       "      <td>2.1315</td>\n",
       "      <td>7.9501</td>\n",
       "      <td>13.9981</td>\n",
       "      <td>-12.6789</td>\n",
       "      <td>-0.7712</td>\n",
       "      <td>5.9889</td>\n",
       "      <td>2.1959</td>\n",
       "      <td>-16.2435</td>\n",
       "      <td>10.9816</td>\n",
       "      <td>-3.2509</td>\n",
       "      <td>7.8550</td>\n",
       "      <td>11.2951</td>\n",
       "      <td>14.3999</td>\n",
       "      <td>2.3497</td>\n",
       "      <td>6.4171</td>\n",
       "      <td>...</td>\n",
       "      <td>11.6782</td>\n",
       "      <td>5.3495</td>\n",
       "      <td>5.1066</td>\n",
       "      <td>17.7215</td>\n",
       "      <td>-5.1919</td>\n",
       "      <td>19.0355</td>\n",
       "      <td>2.7651</td>\n",
       "      <td>-7.4054</td>\n",
       "      <td>6.3445</td>\n",
       "      <td>5.4183</td>\n",
       "      <td>8.6643</td>\n",
       "      <td>-10.3910</td>\n",
       "      <td>21.0844</td>\n",
       "      <td>4.1428</td>\n",
       "      <td>4.7056</td>\n",
       "      <td>8.5329</td>\n",
       "      <td>9.4661</td>\n",
       "      <td>9.6265</td>\n",
       "      <td>9.0617</td>\n",
       "      <td>4.3694</td>\n",
       "      <td>-1.1399</td>\n",
       "      <td>10.9190</td>\n",
       "      <td>-3.9144</td>\n",
       "      <td>8.1388</td>\n",
       "      <td>16.4554</td>\n",
       "      <td>4.7923</td>\n",
       "      <td>9.1281</td>\n",
       "      <td>-21.7322</td>\n",
       "      <td>18.6375</td>\n",
       "      <td>0.1734</td>\n",
       "      <td>5.9215</td>\n",
       "      <td>7.9676</td>\n",
       "      <td>2.3405</td>\n",
       "      <td>1.1482</td>\n",
       "      <td>23.2168</td>\n",
       "      <td>-2.0105</td>\n",
       "      <td>3.7600</td>\n",
       "      <td>9.4513</td>\n",
       "      <td>17.4105</td>\n",
       "      <td>-14.6897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>train_63</td>\n",
       "      <td>1</td>\n",
       "      <td>7.7072</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>9.9974</td>\n",
       "      <td>8.3524</td>\n",
       "      <td>9.2886</td>\n",
       "      <td>-13.3627</td>\n",
       "      <td>6.0425</td>\n",
       "      <td>10.1108</td>\n",
       "      <td>1.3999</td>\n",
       "      <td>6.6710</td>\n",
       "      <td>-5.1448</td>\n",
       "      <td>11.0835</td>\n",
       "      <td>14.0503</td>\n",
       "      <td>9.0144</td>\n",
       "      <td>8.8346</td>\n",
       "      <td>14.8138</td>\n",
       "      <td>5.0951</td>\n",
       "      <td>-1.6585</td>\n",
       "      <td>-2.3445</td>\n",
       "      <td>24.4873</td>\n",
       "      <td>9.4153</td>\n",
       "      <td>23.5101</td>\n",
       "      <td>9.3373</td>\n",
       "      <td>2.1534</td>\n",
       "      <td>11.1901</td>\n",
       "      <td>13.2911</td>\n",
       "      <td>-5.2299</td>\n",
       "      <td>-2.5387</td>\n",
       "      <td>4.1148</td>\n",
       "      <td>9.1110</td>\n",
       "      <td>-14.6391</td>\n",
       "      <td>12.8946</td>\n",
       "      <td>-0.9446</td>\n",
       "      <td>6.1225</td>\n",
       "      <td>10.8937</td>\n",
       "      <td>6.4271</td>\n",
       "      <td>0.7646</td>\n",
       "      <td>7.0633</td>\n",
       "      <td>...</td>\n",
       "      <td>22.5924</td>\n",
       "      <td>5.3977</td>\n",
       "      <td>3.3740</td>\n",
       "      <td>13.0953</td>\n",
       "      <td>3.0745</td>\n",
       "      <td>16.6974</td>\n",
       "      <td>2.0576</td>\n",
       "      <td>5.5558</td>\n",
       "      <td>3.1795</td>\n",
       "      <td>6.0669</td>\n",
       "      <td>2.5841</td>\n",
       "      <td>7.0790</td>\n",
       "      <td>13.1299</td>\n",
       "      <td>2.2445</td>\n",
       "      <td>23.1283</td>\n",
       "      <td>8.9796</td>\n",
       "      <td>-13.6216</td>\n",
       "      <td>10.5579</td>\n",
       "      <td>6.9716</td>\n",
       "      <td>2.9384</td>\n",
       "      <td>-7.2040</td>\n",
       "      <td>9.7592</td>\n",
       "      <td>15.1720</td>\n",
       "      <td>5.2335</td>\n",
       "      <td>27.6213</td>\n",
       "      <td>-9.2779</td>\n",
       "      <td>5.1130</td>\n",
       "      <td>-13.0577</td>\n",
       "      <td>10.0679</td>\n",
       "      <td>1.9046</td>\n",
       "      <td>1.5832</td>\n",
       "      <td>5.0039</td>\n",
       "      <td>3.8814</td>\n",
       "      <td>7.4241</td>\n",
       "      <td>21.4844</td>\n",
       "      <td>-0.8297</td>\n",
       "      <td>-3.0468</td>\n",
       "      <td>7.5790</td>\n",
       "      <td>15.7685</td>\n",
       "      <td>5.4769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>train_65</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5358</td>\n",
       "      <td>-2.5439</td>\n",
       "      <td>8.7394</td>\n",
       "      <td>6.7548</td>\n",
       "      <td>14.4099</td>\n",
       "      <td>-3.8724</td>\n",
       "      <td>5.1584</td>\n",
       "      <td>15.8381</td>\n",
       "      <td>5.8204</td>\n",
       "      <td>9.0358</td>\n",
       "      <td>-10.7504</td>\n",
       "      <td>7.7587</td>\n",
       "      <td>13.6021</td>\n",
       "      <td>4.2864</td>\n",
       "      <td>8.1159</td>\n",
       "      <td>14.0805</td>\n",
       "      <td>5.1303</td>\n",
       "      <td>4.0575</td>\n",
       "      <td>13.8723</td>\n",
       "      <td>16.9321</td>\n",
       "      <td>14.0983</td>\n",
       "      <td>2.2106</td>\n",
       "      <td>3.9192</td>\n",
       "      <td>2.8896</td>\n",
       "      <td>9.2493</td>\n",
       "      <td>13.5054</td>\n",
       "      <td>-0.4275</td>\n",
       "      <td>-3.2016</td>\n",
       "      <td>6.9686</td>\n",
       "      <td>0.8920</td>\n",
       "      <td>-16.6872</td>\n",
       "      <td>12.0119</td>\n",
       "      <td>0.8354</td>\n",
       "      <td>13.6590</td>\n",
       "      <td>11.7710</td>\n",
       "      <td>8.6702</td>\n",
       "      <td>2.3624</td>\n",
       "      <td>6.5731</td>\n",
       "      <td>...</td>\n",
       "      <td>25.3377</td>\n",
       "      <td>5.8066</td>\n",
       "      <td>5.2234</td>\n",
       "      <td>11.3098</td>\n",
       "      <td>-6.7546</td>\n",
       "      <td>30.5059</td>\n",
       "      <td>2.6117</td>\n",
       "      <td>-3.3249</td>\n",
       "      <td>4.7747</td>\n",
       "      <td>6.1149</td>\n",
       "      <td>2.2550</td>\n",
       "      <td>-0.5348</td>\n",
       "      <td>4.5535</td>\n",
       "      <td>-7.2104</td>\n",
       "      <td>19.7545</td>\n",
       "      <td>9.1091</td>\n",
       "      <td>-0.4833</td>\n",
       "      <td>13.7686</td>\n",
       "      <td>3.8680</td>\n",
       "      <td>6.3190</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>12.9782</td>\n",
       "      <td>2.5409</td>\n",
       "      <td>13.7491</td>\n",
       "      <td>11.9410</td>\n",
       "      <td>2.8503</td>\n",
       "      <td>8.2088</td>\n",
       "      <td>-4.6266</td>\n",
       "      <td>10.2542</td>\n",
       "      <td>1.5517</td>\n",
       "      <td>4.6648</td>\n",
       "      <td>6.4227</td>\n",
       "      <td>3.4025</td>\n",
       "      <td>-4.0882</td>\n",
       "      <td>14.1174</td>\n",
       "      <td>-0.2472</td>\n",
       "      <td>5.3847</td>\n",
       "      <td>8.6949</td>\n",
       "      <td>15.1340</td>\n",
       "      <td>3.8449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>train_71</td>\n",
       "      <td>1</td>\n",
       "      <td>6.7547</td>\n",
       "      <td>2.5973</td>\n",
       "      <td>14.2141</td>\n",
       "      <td>8.3514</td>\n",
       "      <td>7.4942</td>\n",
       "      <td>-1.3055</td>\n",
       "      <td>4.2336</td>\n",
       "      <td>15.0243</td>\n",
       "      <td>-1.8922</td>\n",
       "      <td>9.1282</td>\n",
       "      <td>-11.4237</td>\n",
       "      <td>-8.6036</td>\n",
       "      <td>13.9394</td>\n",
       "      <td>3.6480</td>\n",
       "      <td>5.0944</td>\n",
       "      <td>14.4429</td>\n",
       "      <td>9.2046</td>\n",
       "      <td>-4.3236</td>\n",
       "      <td>24.2658</td>\n",
       "      <td>13.5924</td>\n",
       "      <td>16.0715</td>\n",
       "      <td>33.3687</td>\n",
       "      <td>9.6420</td>\n",
       "      <td>2.2948</td>\n",
       "      <td>4.1684</td>\n",
       "      <td>13.3240</td>\n",
       "      <td>-2.8896</td>\n",
       "      <td>-3.2697</td>\n",
       "      <td>3.9168</td>\n",
       "      <td>4.3874</td>\n",
       "      <td>-7.1606</td>\n",
       "      <td>10.7617</td>\n",
       "      <td>-1.5133</td>\n",
       "      <td>7.8835</td>\n",
       "      <td>10.7454</td>\n",
       "      <td>9.3655</td>\n",
       "      <td>3.7260</td>\n",
       "      <td>4.6904</td>\n",
       "      <td>...</td>\n",
       "      <td>29.8664</td>\n",
       "      <td>5.7714</td>\n",
       "      <td>4.3278</td>\n",
       "      <td>15.8608</td>\n",
       "      <td>2.9925</td>\n",
       "      <td>13.3188</td>\n",
       "      <td>3.2917</td>\n",
       "      <td>-3.4176</td>\n",
       "      <td>6.6747</td>\n",
       "      <td>5.0565</td>\n",
       "      <td>11.9429</td>\n",
       "      <td>7.7473</td>\n",
       "      <td>8.6512</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>20.4828</td>\n",
       "      <td>8.9715</td>\n",
       "      <td>-3.0613</td>\n",
       "      <td>12.5752</td>\n",
       "      <td>-14.9327</td>\n",
       "      <td>7.9488</td>\n",
       "      <td>9.3111</td>\n",
       "      <td>7.3559</td>\n",
       "      <td>3.7889</td>\n",
       "      <td>15.9203</td>\n",
       "      <td>11.2587</td>\n",
       "      <td>-0.7797</td>\n",
       "      <td>5.1381</td>\n",
       "      <td>6.1459</td>\n",
       "      <td>13.8773</td>\n",
       "      <td>-0.0899</td>\n",
       "      <td>1.4677</td>\n",
       "      <td>3.5935</td>\n",
       "      <td>2.0013</td>\n",
       "      <td>1.5777</td>\n",
       "      <td>18.2820</td>\n",
       "      <td>-4.3408</td>\n",
       "      <td>6.8869</td>\n",
       "      <td>9.3567</td>\n",
       "      <td>18.9013</td>\n",
       "      <td>13.3447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_code  target    var_0   ...     var_197  var_198  var_199\n",
       "13  train_13       1  16.3699   ...      9.0419  15.6064 -10.8529\n",
       "29  train_29       1   5.3301   ...      9.4513  17.4105 -14.6897\n",
       "63  train_63       1   7.7072   ...      7.5790  15.7685   5.4769\n",
       "65  train_65       1  10.5358   ...      8.6949  15.1340   3.8449\n",
       "71  train_71       1   6.7547   ...      9.3567  18.9013  13.3447\n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the mask to filter out 1s . \n",
    "y = df_train['target']\n",
    "y.head()\n",
    "x = df_train[y > 0].copy()\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "11e4e9f6ee5aabcb0783768e03e786f141677538"
   },
   "outputs": [],
   "source": [
    "def augment(x,y,t=2):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2bac2796bb647af39fb8bdfbe295817f7f4c8dce"
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"auc\",\n",
    "    \"boosting\": 'gbdt',\n",
    "    \"max_depth\" : -1,\n",
    "    \"num_leaves\" : 13,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\" : 0.4,\n",
    "    \"feature_fraction\" : 0.05,\n",
    "    \"min_data_in_leaf\": 80,\n",
    "    \"min_sum_hessian_in_leaf\" : 10,\n",
    "    \"tree_learner\": \"serial\",\n",
    "    \"boost_from_average\": \"false\",\n",
    "    #\"lambda_l1\" : 5,\n",
    "    #\"lambda_l2\" : 5,\n",
    "    \"bagging_seed\" : random_state,\n",
    "    \"verbosity\" : 1,\n",
    "    \"seed\": random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d15c7bb080a4642a3b331ae24830cdb6b7b67cde"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "oof = df_train[['ID_code', 'target']]\n",
    "oof['predict'] = 0\n",
    "predictions = df_test[['ID_code']]\n",
    "val_aucs = []\n",
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "20a221b86ee4fe50c371767c63a72f225752213c"
   },
   "outputs": [],
   "source": [
    "features = [col for col in df_train.columns if col not in ['target', 'ID_code']]\n",
    "X_test = df_test[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e48a4e9baa428a0280d526df9d0128f34a7fbe85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.892958\tvalid_1's auc: 0.881779\n",
      "[2000]\ttraining's auc: 0.901489\tvalid_1's auc: 0.88863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0925aa658cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                         \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                        )\n\u001b[1;32m     23\u001b[0m         \u001b[0mp_valid\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n",
    "    X_train, y_train = df_train.iloc[trn_idx][features], df_train.iloc[trn_idx]['target']\n",
    "    X_valid, y_valid = df_train.iloc[val_idx][features], df_train.iloc[val_idx]['target']\n",
    "    \n",
    "    N = 5\n",
    "    p_valid,yp = 0,0\n",
    "    for i in range(N):\n",
    "        X_t, y_t = augment(X_train.values, y_train.values)\n",
    "        X_t = pd.DataFrame(X_t)\n",
    "        X_t = X_t.add_prefix('var_')\n",
    "    \n",
    "        trn_data = lgb.Dataset(X_t, label=y_t)\n",
    "        val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        evals_result = {}\n",
    "        lgb_clf = lgb.train(lgb_params,\n",
    "                        trn_data,\n",
    "                        100000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        early_stopping_rounds=3000,\n",
    "                        verbose_eval = 1000,\n",
    "                        evals_result=evals_result\n",
    "                       )\n",
    "        p_valid += lgb_clf.predict(X_valid)\n",
    "        yp += lgb_clf.predict(X_test)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    oof['predict'][val_idx] = p_valid/N\n",
    "    val_score = roc_auc_score(y_valid, p_valid)\n",
    "    val_aucs.append(val_score)\n",
    "    \n",
    "    predictions['fold{}'.format(fold+1)] = yp/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see what this augmentation does to our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of 1s in original data : {} / {} \".format(np.sum(y_train) , len(y_train)))\n",
    "print(\"Percentage of 1s in original data : {}\".format(np.sum(y_train)*100.0/len(y_train)))\n",
    "\n",
    "\n",
    "print(\"Percentage of 1s in augmented data : {}\".format(np.sum(y_t)*100.0/len(y_t)))\n",
    "print(\"Distribution of 1s in augmented data : {} / {} \".format(np.sum(y_t) , len(y_t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So How the augmentation was done ? \n",
    "\n",
    "    * X : Original  : 200,000\n",
    "    * Xs : Ones     ~  20,000\n",
    "    * Xn : Zeros    ~ 180,000  \n",
    "\n",
    "    ### X_Final = X + 3\\*Xs + 2\\*Xn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof : ? \n",
    "\n",
    "1s = 20k(X) + 3\\*20k(Xs) = 80k\n",
    "Total = 20k(X) + 3\\*20k(Xs) + 2\\*180k(Xn)\n",
    "\n",
    "i.e. , \n",
    "    80k / 520 k = \n",
    "    14.6 % (Approx. , the one we found above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hence this technique is more like oversampling , but , here we oversample BOTH classses , rather than just one.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "055d26ed03bc1b21bade15f91fc7964cebf39fbc"
   },
   "outputs": [],
   "source": [
    "mean_auc = np.mean(val_aucs)\n",
    "std_auc = np.std(val_aucs)\n",
    "all_auc = roc_auc_score(oof['target'], oof['predict'])\n",
    "print(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4a99e866bcecb5cd7655fc21e2fc0409a686268"
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('LightGBM Features (averaged over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f698e19c1c399db90cb98e698a08cc663dd8319"
   },
   "outputs": [],
   "source": [
    "# submission\n",
    "predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\n",
    "predictions.to_csv('lgb_all_predictions.csv', index=None)\n",
    "sub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\n",
    "sub_df[\"target\"] = predictions['target']\n",
    "sub_df.to_csv(\"lgb_submission.csv\", index=False)\n",
    "oof.to_csv('lgb_oof.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
